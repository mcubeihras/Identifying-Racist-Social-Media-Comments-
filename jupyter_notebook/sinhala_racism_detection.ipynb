{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running in Google Colaboratory\n",
    "# If you are running in local ignore this.\n",
    "# Authenticate user and create folder \"sinhala_racism_detection\" to save results\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "parent_dir = '/content/gdrive/My Drive/sinhala_racism_detection'\n",
    "if not os.path.exists(parent_dir):\n",
    "    os.makedirs(parent_dir)\n",
    "\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running local use this \"../data-set/final-data-set.csv\"\n",
    "DATA_SET_PATH = \"https://github.com/renuka-fernando/sinhalese_language_racism_detection/raw/master/data-set/final-data-set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_TWEET_ID = 1\n",
    "DATA_SET_USER_ID = 2\n",
    "DATA_SET_TEXT = 4\n",
    "DATA_SET_CLASS = 5\n",
    "\n",
    "MAX_WORD_COUNT = 60\n",
    "\n",
    "DATA_SET_CLASSES = {\n",
    "    'Neutral': [0, 0, 1],\n",
    "    'Racist': [0, 1, 0],\n",
    "    'Sexism': [1, 0, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinhalese_chars = [\n",
    "    \"අ\", \"ආ\", \"ඇ\", \"ඈ\", \"ඉ\", \"ඊ\",\n",
    "    \"උ\", \"ඌ\", \"ඍ\", \"ඎ\", \"ඏ\", \"ඐ\",\n",
    "    \"එ\", \"ඒ\", \"ඓ\", \"ඔ\", \"ඕ\", \"ඖ\",\n",
    "    \"ං\", \"ඃ\",\n",
    "    \"ක\", \"ඛ\", \"ග\", \"ඝ\", \"ඞ\", \"ඟ\",\n",
    "    \"ච\", \"ඡ\", \"ජ\", \"ඣ\", \"ඤ\", \"ඥ\", \"ඦ\",\n",
    "    \"ට\", \"ඨ\", \"ඩ\", \"ඪ\", \"ණ\", \"ඬ\",\n",
    "    \"ත\", \"ථ\", \"ද\", \"ධ\", \"න\", \"ඳ\",\n",
    "    \"ප\", \"ඵ\", \"බ\", \"භ\", \"ම\", \"ඹ\",\n",
    "    \"ය\", \"ර\", \"ල\", \"ව\",\n",
    "    \"ශ\", \"ෂ\", \"ස\", \"හ\", \"ළ\", \"ෆ\",\n",
    "    \"෴\", \"\\u200d\"\n",
    "]\n",
    "# \"\\u200d\" is used with \"යංශය\" - කාව්‍ය, \"රේඵය\" - වර්‍තමාන, \"Both\" - මහාචාර්‍ය්‍ය, \"රකාරාංශය\" - මුද්‍රණය\n",
    "\n",
    "sinhalese_vowel_signs = [\"්\", \"ා\", \"ැ\", \"ෑ\", \"ි\", \"ී\", \"ු\", \"ූ\", \"ෘ\", \"ෙ\", \"ේ\", \"ෛ\", \"ො\", \"ෝ\",\n",
    "                         \"ෞ\", \"ෟ\", \"ෲ\", \"ෳ\", \"ර්‍\"]\n",
    "\n",
    "# dictionary that maps wrong usage of vowels to correct vowels\n",
    "vowel_sign_fix_dict = {\n",
    "    \"ෙ\" + \"්\": \"ේ\",\n",
    "    \"්\" + \"ෙ\": \"ේ\",\n",
    "\n",
    "    \"ෙ\" + \"ා\": \"ො\",\n",
    "    \"ා\" + \"ෙ\": \"ො\",\n",
    "\n",
    "    \"ේ\" + \"ා\": \"ෝ\",\n",
    "    \"ො\" + \"්\": \"ෝ\",\n",
    "\n",
    "    \"ෙෙ\": \"ෛ\",\n",
    "    \"ෘෘ\": \"ෲ\",\n",
    "\n",
    "    \"ෙ\" + \"ෟ\": \"ෞ\",\n",
    "    \"ෟ\" + \"ෙ\": \"ෞ\",\n",
    "\n",
    "    \"ි\" + \"ී\": \"ී\",\n",
    "    \"ී\" + \"ි\": \"ී\",\n",
    "\n",
    "    # duplicating same symbol\n",
    "    \"ේ\" + \"්\": \"ේ\",\n",
    "    \"ේ\" + \"ෙ\": \"ේ\",\n",
    "\n",
    "    \"ො\" + \"ා\": \"ො\",\n",
    "    \"ො\" + \"ෙ\": \"ො\",\n",
    "\n",
    "    \"ෝ\" + \"ා\": \"ෝ\",\n",
    "    \"ෝ\" + \"්\": \"ෝ\",\n",
    "    \"ෝ\" + \"ෙ\": \"ෝ\",\n",
    "    \"ෝ\" + \"ේ\": \"ෝ\",\n",
    "    \"ෝ\" + \"ො\": \"ෝ\",\n",
    "\n",
    "    \"ෞ\" + \"ෟ\": \"ෞ\",\n",
    "    \"ෞ\" + \"ෙ\": \"ෞ\",\n",
    "\n",
    "    # special cases - may be typing mistakes\n",
    "    \"ො\" + \"ෟ\": \"ෞ\",\n",
    "    \"ෟ\" + \"ො\": \"ෞ\",\n",
    "}\n",
    "\n",
    "simplify_characters_dict = {\n",
    "    # Consonant\n",
    "    \"ඛ\": \"ක\",\n",
    "    \"ඝ\": \"ග\",\n",
    "    \"ඟ\": \"ග\",\n",
    "    \"ඡ\": \"ච\",\n",
    "    \"ඣ\": \"ජ\",\n",
    "    \"ඦ\": \"ජ\",\n",
    "    \"ඤ\": \"ඥ\",\n",
    "    \"ඨ\": \"ට\",\n",
    "    \"ඪ\": \"ඩ\",\n",
    "    \"ණ\": \"න\",\n",
    "    \"ඳ\": \"ද\",\n",
    "    \"ඵ\": \"ප\",\n",
    "    \"භ\": \"බ\",\n",
    "    \"ඹ\": \"බ\",\n",
    "    \"ශ\": \"ෂ\",\n",
    "    \"ළ\": \"ල\",\n",
    "\n",
    "    # Vowels\n",
    "    \"ආ\": \"අ\",\n",
    "    \"ඈ\": \"ඇ\",\n",
    "    \"ඊ\": \"ඉ\",\n",
    "    \"ඌ\": \"උ\",\n",
    "    \"ඒ\": \"එ\",\n",
    "    \"ඕ\": \"ඔ\",\n",
    "\n",
    "    \"ා\": \"\",\n",
    "    \"ෑ\": \"ැ\",\n",
    "    \"ී\": \"ි\",\n",
    "    \"ූ\": \"ු\",\n",
    "    \"ේ\": \"ෙ\",\n",
    "    \"ෝ\": \"ො\",\n",
    "    \"ෲ\": \"ෘ\"\n",
    "}\n",
    "\n",
    "\n",
    "def is_sinhalese_letter(char: str) -> bool:\n",
    "    return char in sinhalese_chars\n",
    "\n",
    "\n",
    "def is_sinhalese_vowel(char: str) -> bool:\n",
    "    return char in sinhalese_vowel_signs\n",
    "\n",
    "\n",
    "def get_fixed_vowel(vowel: str) -> str:\n",
    "    return vowel_sign_fix_dict[vowel]\n",
    "\n",
    "\n",
    "def get_simplified_character(character: str) -> str:\n",
    "    if len(character) != 1:\n",
    "        raise TypeError(\"character should be a string with length 1\")\n",
    "    try:\n",
    "        return simplify_characters_dict[character]\n",
    "    except KeyError:\n",
    "        return character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q emoji\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "def replace_url(text: str) -> str:\n",
    "    \"\"\"\n",
    "    replace URL of a text\n",
    "    :param text: text to replace urls\n",
    "    :return: url removed text\n",
    "    \"\"\"\n",
    "    return re.sub(r'(http://www\\.|https://www\\.|http://|https://)[a-z0-9]+([\\-.]{1}[a-z0-9A-Z/]+)*', '', text)\n",
    "\n",
    "\n",
    "def remove_retweet_state(text: str) -> str:\n",
    "    \"\"\"\n",
    "    remove retweet states in the beginning such as \"RT @sam92ky: \"\n",
    "    :param text: text\n",
    "    :return: text removed retweets state\n",
    "    \"\"\"\n",
    "    return re.sub(r'^RT @\\w*: ', '', text)\n",
    "\n",
    "\n",
    "def replace_mention(text: str) -> str:\n",
    "    return re.sub(r'@\\w*', 'PERSON', text)\n",
    "\n",
    "\n",
    "def split_tokens(text: str) -> list:\n",
    "    \"\"\"\n",
    "    tokenize text\n",
    "    :param text: text\n",
    "    :return: token list\n",
    "    \"\"\"\n",
    "    # text characters to split is from: https://github.com/madurangasiriwardena/corpus.sinhala.tools\n",
    "    emojis = ''.join(emj for emj in emoji.UNICODE_EMOJI.keys())\n",
    "    return [token for token in\n",
    "            re.split(r'[.…,‌ ¸‚\\\"/|—¦”‘\\'“’´!@#$%^&*+\\-£?˜()\\[\\]{\\}:;–Ê  �‪‬‏0123456789' + emojis + ']', text)\n",
    "            if token != \"\"]\n",
    "\n",
    "\n",
    "def set_spaces_among_emojis(text: str) -> str:\n",
    "    \"\"\"\n",
    "    make spaces among emojis to tokenize them\n",
    "    :param text: text to be modified\n",
    "    :return: modified text\n",
    "    \"\"\"\n",
    "    modified_text = \"\"\n",
    "    for c in text:\n",
    "        modified_text += c\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            modified_text += \" \"\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "\n",
    "def simplify_sinhalese_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    simplify\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    modified_text = \"\"\n",
    "    for c in text:\n",
    "        modified_text += get_simplified_character(c)\n",
    "    return modified_text\n",
    "\n",
    "\n",
    "def stem_word(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Stemming words\n",
    "    :param word: word\n",
    "    :return: stemmed word\n",
    "    \"\"\"\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "\n",
    "    # remove 'ට'\n",
    "    if word[-1] == 'ට':\n",
    "        return word[:-1]\n",
    "\n",
    "    # remove 'ද'\n",
    "    if word[-1] == 'ද':\n",
    "        return word[:-1]\n",
    "\n",
    "    # remove 'ටත්'\n",
    "    if word[-3:] == 'ටත්':\n",
    "        return word[:-3]\n",
    "\n",
    "    # remove 'එක්'\n",
    "    if word[-3:] == 'ෙක්':\n",
    "        return word[:-3]\n",
    "\n",
    "    # remove 'එ'\n",
    "    if word[-1:] == 'ෙ':\n",
    "        return word[:-1]\n",
    "\n",
    "    # remove 'ක්'\n",
    "    if word[-2:] == 'ක්':\n",
    "        return word[:-2]\n",
    "\n",
    "    # remove 'ගෙ' (instead of ගේ because this step comes after simplifying text)\n",
    "    if word[-2:] == 'ගෙ':\n",
    "        return word[:-2]\n",
    "\n",
    "    # else\n",
    "    return word\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    # todo: add stem_word(token) and simplify_sinhalese_text methods\n",
    "    return [stem_word(token) for token in split_tokens(replace_url(replace_mention(\n",
    "        simplify_sinhalese_text(remove_retweet_state(text.strip('\"')).lower()))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_corpus(corpus: list) -> list:\n",
    "    return [tokenize(text) for text in corpus]\n",
    "\n",
    "\n",
    "def transform_class_to_one_hot_representation(classes: list):\n",
    "    return np.array([DATA_SET_CLASSES[cls] for cls in classes])\n",
    "\n",
    "\n",
    "def build_dictionary(corpus_token: list) -> dict:\n",
    "    word_frequency = {}\n",
    "    dictionary = {}\n",
    "\n",
    "    for tweet in corpus_token:\n",
    "        for token in tweet:\n",
    "            if token in word_frequency:\n",
    "                word_frequency[token] += 1\n",
    "            else:\n",
    "                word_frequency[token] = 1\n",
    "\n",
    "    frequencies = list(word_frequency.values())\n",
    "    unique_words = list(word_frequency.keys())\n",
    "\n",
    "    # sort words by its frequency\n",
    "    frequency_indexes = np.argsort(frequencies)[::-1]  # reverse for descending\n",
    "    for index, frequency_index in enumerate(frequency_indexes):\n",
    "        # 0 is not used and 1 is for UNKNOWN\n",
    "        dictionary[unique_words[frequency_index]] = index + 2\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def transform_to_dictionary_values(corpus_token: list, dictionary: dict) -> list:\n",
    "    x_corpus = []\n",
    "    for tweet in corpus_token:\n",
    "        # 1 is for unknown (not in dictionary)\n",
    "        x_corpus.append([dictionary[token] if token in dictionary else 1 for token in tweet])\n",
    "\n",
    "    return x_corpus\n",
    "\n",
    "\n",
    "def get_calculated_user_profile(user_ids: list, classes: list) -> dict:\n",
    "    user_profile = {}\n",
    "    user_tweets_count = {}\n",
    "\n",
    "    for i in range(len(user_ids)):\n",
    "        # count tweets with class\n",
    "        try:\n",
    "            user_profile[user_ids[i], classes[i]] += 1\n",
    "        except KeyError:\n",
    "            user_profile[user_ids[i], classes[i]] = 1\n",
    "\n",
    "        # count tweets\n",
    "        try:\n",
    "            user_tweets_count[user_ids[i]] += 1\n",
    "        except KeyError:\n",
    "            user_tweets_count[user_ids[i]] = 1\n",
    "\n",
    "    # calculate mean\n",
    "    for profile in user_profile.keys():\n",
    "        user_profile[profile] /= user_tweets_count[profile[0]]\n",
    "\n",
    "    return user_profile\n",
    "\n",
    "\n",
    "def append_user_profile_features(x_corpus: list, user_ids: list, user_profile: dict) -> list:\n",
    "    \"\"\"\n",
    "    append neutral, racism, sexism user profile probability feature to the end of each sentence\n",
    "    :param x_corpus: corpus with coded to integers\n",
    "    :param user_ids: list of user ids in the order of x_corpus\n",
    "    :param user_profile: user profile with user's probabilities for neutral, racism, sexism\n",
    "    :return: appended x_corpus\n",
    "    \"\"\"\n",
    "    for i in range(len(x_corpus)):\n",
    "        uid = user_ids[i]\n",
    "        try:\n",
    "            neutral = user_profile[uid, \"Neutral\"]\n",
    "        except KeyError:\n",
    "            neutral = 0\n",
    "\n",
    "        try:\n",
    "            racism = user_profile[uid, \"Racist\"]\n",
    "        except KeyError:\n",
    "            racism = 0\n",
    "\n",
    "        try:\n",
    "            sexism = user_profile[uid, \"Sexism\"]\n",
    "        except KeyError:\n",
    "            sexism = 0\n",
    "\n",
    "        x_corpus[i].append(int(neutral * 1000))\n",
    "        x_corpus[i].append(int(racism * 1000))\n",
    "        x_corpus[i].append(int(sexism * 1000))\n",
    "\n",
    "    return x_corpus\n",
    "\n",
    "\n",
    "def create_next_results_folder():\n",
    "    \"\"\"\n",
    "    Create the next results folder and returns the directory name\n",
    "    :return: directory name\n",
    "    \"\"\"\n",
    "    result_no = 0\n",
    "    directory = \"results_%d\" % result_no\n",
    "\n",
    "    while os.path.exists(directory):\n",
    "        result_no += 1\n",
    "        directory = \"results_%d\" % result_no\n",
    "\n",
    "    os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "\n",
    "def get_last_results_folder():\n",
    "    \"\"\"\n",
    "    Return last created results directory\n",
    "    :return: last created results directory\n",
    "    \"\"\"\n",
    "    result_no = 0\n",
    "    directory = \"results_%d\" % result_no\n",
    "\n",
    "    while os.path.exists(directory):\n",
    "        result_no += 1\n",
    "        directory = \"results_%d\" % result_no\n",
    "\n",
    "    return \"results_%d\" % (result_no - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s %(asctime)s: %(message)s', level=logging.INFO)\n",
    "\n",
    "data_frame = pd.read_csv(DATA_SET_PATH)\n",
    "data_set = data_frame.values\n",
    "\n",
    "# for id, tweet in enumerate(data_set[:, :]):\n",
    "#     print(id, tweet[-1])\n",
    "\n",
    "# for index, tweet in enumerate(data_frame.text):\n",
    "#     print(index, tweet)\n",
    "\n",
    "logging.info(\"Tokenizing the corpus\")\n",
    "corpus_token = tokenize_corpus(data_set[:, DATA_SET_TEXT])\n",
    "\n",
    "logging.info(\"Building the dictionary\")\n",
    "dictionary = build_dictionary(corpus_token)\n",
    "dictionary_length = len(dictionary) + 2  # 0 is not used and 1 is for UNKNOWN\n",
    "\n",
    "# to get sentence back\n",
    "# ' '.join([list(dictionary.keys())[i-2] for i in x_test[0] if i > 1])\n",
    "\n",
    "logging.info(\"Transforming the corpus to dictionary values\")\n",
    "x_corpus = transform_to_dictionary_values(corpus_token, dictionary)\n",
    "\n",
    "y_corpus = transform_class_to_one_hot_representation(data_set[:, DATA_SET_CLASS])\n",
    "user_profile = get_calculated_user_profile(data_set[:, DATA_SET_USER_ID], data_set[:, DATA_SET_CLASS])\n",
    "\n",
    "# add user profile feature to end of the sentence\n",
    "# from: Detecting Offensive Language in Tweets using Deep Learning\n",
    "# by: Georgios K. Pitsilis, Heri Ramampiaro and Helge Langseth\n",
    "max_word_count = MAX_WORD_COUNT + 3\n",
    "x_corpus = append_user_profile_features(x_corpus=x_corpus, user_ids=data_set[:, DATA_SET_USER_ID],\n",
    "                                        user_profile=user_profile)\n",
    "\n",
    "# padding with zeros if not enough and else drop left-side words\n",
    "x_corpus = sequence.pad_sequences(x_corpus, maxlen=max_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################## Deep Neural Network ###################### #\n",
    "FOLDS_COUNT = 5\n",
    "MAX_EPOCHS = 15\n",
    "VALIDATION_TEST_SIZE = 0.12\n",
    "\n",
    "# splitting data for 5-fold cross validation\n",
    "k_fold = StratifiedKFold(n_splits=FOLDS_COUNT, shuffle=True, random_state=18)\n",
    "# to split, raw format (integer) is required\n",
    "y_corpus_raw = [0 if cls[2] == 1 else (1 if cls[1] == 1 else 2) for cls in y_corpus]\n",
    "\n",
    "directory = create_next_results_folder()  # directory for saving results\n",
    "logging.info(\"created the directory: %s\" % directory)\n",
    "\n",
    "fold = 0\n",
    "for train_n_validation_indexes, test_indexes in k_fold.split(x_corpus, y_corpus_raw):\n",
    "    x_train_n_validation = x_corpus[train_n_validation_indexes]\n",
    "    y_train_n_validation = y_corpus[train_n_validation_indexes]\n",
    "    x_test = x_corpus[test_indexes]\n",
    "    y_test = y_corpus[test_indexes]\n",
    "\n",
    "    # train and validation data sets\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train_n_validation, y_train_n_validation,\n",
    "                                                          test_size=VALIDATION_TEST_SIZE, random_state=94)\n",
    "\n",
    "    # ################## Deep Neural Network Model ###################### #\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n",
    "    model.add(LSTM(600))\n",
    "    model.add(Dense(units=max_word_count, activation='tanh', kernel_regularizer=regularizers.l2(0.04),\n",
    "                    activity_regularizer=regularizers.l2(0.015)))\n",
    "    model.add(Dense(units=max_word_count, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    # ################## Deep Neural Network Model ###################### #\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_loss = 100000\n",
    "    best_epoch = 0\n",
    "\n",
    "    epoch_history = {\n",
    "        'acc': [],\n",
    "        'val_acc': [],\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "    }\n",
    "\n",
    "    # for each epoch\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        logging.info(\"Fold: %d/%d\" % (fold, FOLDS_COUNT))\n",
    "        logging.info(\"Epoch: %d/%d\" % (epoch, MAX_EPOCHS))\n",
    "        history = model.fit(x=x_train, y=y_train, epochs=1, batch_size=1, validation_data=(x_valid, y_valid),\n",
    "                            verbose=1, shuffle=False)\n",
    "\n",
    "        # get validation (test) accuracy and loss\n",
    "        accuracy = history.history['val_acc'][0]\n",
    "        loss = history.history['val_loss'][0]\n",
    "\n",
    "        # set epochs' history\n",
    "        epoch_history['acc'].append(history.history['acc'][0])\n",
    "        epoch_history['val_acc'].append(history.history['val_acc'][0])\n",
    "        epoch_history['loss'].append(history.history['loss'][0])\n",
    "        epoch_history['val_loss'].append(history.history['val_loss'][0])\n",
    "\n",
    "        # select best epoch and save to disk\n",
    "        if accuracy >= best_accuracy and loss < best_loss + 0.01:\n",
    "            logging.info(\"Saving model\")\n",
    "            model.save(\"%s/model_fold_%d.h5\" % (directory, fold))\n",
    "\n",
    "            best_accuracy = accuracy\n",
    "            best_loss = loss\n",
    "            best_epoch = epoch\n",
    "        # end of epoch\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(epoch_history['acc'])\n",
    "    plt.plot(epoch_history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(\"%s/plot_model_accuracy_%d\" % (directory, fold))\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(epoch_history['loss'])\n",
    "    plt.plot(epoch_history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(\"%s/plot_model_loss_%d\" % (directory, fold))\n",
    "    plt.show()\n",
    "\n",
    "    # Saving evolution history of epochs in this fold\n",
    "    f = open(\"%s/history_fold_%d.txt\" % (directory, fold), 'w')\n",
    "    f.write(\"best_epoch: %d\\n\" % best_epoch)\n",
    "    f.write(\"epoch,training_accuracy,training_loss,validation_accuracy,validation_loss\\n\")\n",
    "    for i in range(MAX_EPOCHS):\n",
    "        f.write(\"%d,%f,%f,%f,%f\\n\" % (i, epoch_history['acc'][i], epoch_history['loss'][i],\n",
    "                                      epoch_history['val_acc'][i], epoch_history['val_loss'][i]))\n",
    "    f.close()\n",
    "\n",
    "    # load the best model saved on disk\n",
    "    del model\n",
    "    model = load_model(\"%s/model_fold_%d.h5\" % (directory, fold))\n",
    "\n",
    "    evaluation = model.evaluate(x=x_test, y=y_test)\n",
    "    logging.info(\"Accuracy: %f\" % evaluation[1])\n",
    "\n",
    "    prediction = model.predict(x_test)\n",
    "\n",
    "    # save predictions to disk\n",
    "    test_indexes = test_indexes.reshape(test_indexes.shape[0], 1)\n",
    "    tweet_ids = data_set[:, DATA_SET_USER_ID][test_indexes]\n",
    "    true_labels = np.asarray(y_corpus_raw, dtype=int)[test_indexes]\n",
    "    class_1 = prediction[:, 2]\n",
    "    class_2 = prediction[:, 1]\n",
    "    class_3 = prediction[:, 0]\n",
    "    output = np.append(tweet_ids, true_labels, axis=1)\n",
    "    output = np.append(output, class_1.reshape(test_indexes.shape[0], 1), axis=1)\n",
    "    output = np.append(output, class_2.reshape(test_indexes.shape[0], 1), axis=1)\n",
    "    output = np.append(output, class_3.reshape(test_indexes.shape[0], 1), axis=1)\n",
    "\n",
    "    np.savetxt(\"%s/test_set_predicted_output_%d.txt\" % (directory, fold), X=output, fmt=\"%s\", delimiter=\",\")\n",
    "    logging.info(\"Fold: %d - Completed\" % fold)\n",
    "    fold += 1\n",
    "    # end of fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
